{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Prompt Caching in OpenAI and Anthropic\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Monitor OpenAI's automatic prompt caching\n",
    "2. Implement manual prompt caching with Anthropic's API\n",
    "3. Compare the cost savings between different approaches\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set up our API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.50.2)\n",
      "Requirement already satisfied: anthropic in /opt/anaconda3/lib/python3.12/site-packages (0.37.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from anthropic) (0.20.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/lib/python3.12/site-packages (from tokenizers>=0.13.0->anthropic) (0.24.6)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import anthropic\n",
    "import getpass\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OpenAI API key if not already set\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    openai.api_key = getpass.getpass('Enter OpenAI API key: ')\n",
    "else:\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Get Anthropic API key if not already set \n",
    "if not os.getenv('ANTHROPIC_API_KEY'):\n",
    "    anthropic_client = anthropic.Anthropic(api_key=getpass.getpass('Enter Anthropic API key: '))\n",
    "else:\n",
    "    anthropic_client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: OpenAI Automatic Prompt Caching\n",
    "\n",
    "OpenAI automatically caches prompts longer than 1,024 tokens. Let's create a function to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_openai_caching(system_prompt, user_prompt):\n",
    "    \"\"\"Make an OpenAI API call and check cache usage\"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    usage = response.usage\n",
    "    print(f\"Total tokens: {usage.total_tokens}\")\n",
    "    print(f\"Prompt tokens: {usage.prompt_tokens}\")\n",
    "    print(f\"Completion tokens: {usage.completion_tokens}\")\n",
    "    \n",
    "    # Check if prompt_tokens_details exists and has cached_tokens\n",
    "    if hasattr(usage, 'prompt_tokens_details'):\n",
    "        if isinstance(usage.prompt_tokens_details, dict):\n",
    "            cached_tokens = usage.prompt_tokens_details.get('cached_tokens', 0)\n",
    "        else:\n",
    "            cached_tokens = getattr(usage.prompt_tokens_details, 'cached_tokens', 0)\n",
    "    else:\n",
    "        cached_tokens = 0\n",
    "    \n",
    "    print(f\"Cached tokens: {cached_tokens}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with a long system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:\n",
      "Total tokens: 1811\n",
      "Prompt tokens: 1232\n",
      "Completion tokens: 579\n",
      "Cached tokens: 0\n",
      "\n",
      "Second call:\n",
      "Total tokens: 1830\n",
      "Prompt tokens: 1232\n",
      "Completion tokens: 598\n",
      "Cached tokens: 1024\n"
     ]
    }
   ],
   "source": [
    "# Create a long system prompt (>1024 tokens)\n",
    "long_system_prompt = \"\"\"You are an AI assistant specialized in explaining complex topics. Here are 20 examples of good explanations that demonstrate the ideal structure, depth, and clarity we aim for:\n",
    "\n",
    "1. EXAMPLE - Quantum Computing:\n",
    "When explaining quantum computing, start with classical bits (0s and 1s) and contrast them with quantum bits (qubits) that can exist in multiple states simultaneously through superposition. Use the analogy of a coin: a classical bit is like a coin lying flat (definitely heads or tails), while a qubit is like a spinning coin (simultaneously heads and tails until observed). Emphasize how quantum entanglement allows qubits to be interconnected in ways classical bits cannot, exponentially increasing computational power for specific tasks.\n",
    "\n",
    "2. EXAMPLE - Photosynthesis:\n",
    "Break down photosynthesis into its key stages: light-dependent reactions and the Calvin cycle. Describe how chlorophyll molecules capture sunlight energy, converting it into chemical energy (ATP and NADPH). Then explain how this energy is used to convert CO2 into glucose through the Calvin cycle. Use the analogy of a solar-powered factory where sunlight is the power source (light-dependent reactions) and the assembly line (Calvin cycle) builds sugar molecules from carbon dioxide.\n",
    "\n",
    "3. EXAMPLE - Special Relativity:\n",
    "Begin with Einstein's two postulates: the constancy of light speed and the equivalence of physical laws in all inertial reference frames. Use the classic train-and-platform thought experiment to illustrate simultaneity. Explain time dilation using the light clock analogy, showing why moving clocks tick more slowly from a stationary observer's perspective. Connect these concepts to the famous equation E=mcÂ², explaining how mass and energy are different forms of the same thing.\n",
    "\n",
    "4. EXAMPLE - Neural Networks:\n",
    "Compare artificial neural networks to biological brains: neurons are like nodes, synapses like weighted connections. Explain how information flows from input layer through hidden layers to output, with each connection being strengthened or weakened during training (like building muscle memory). Use the analogy of a complex voting system where each neuron \"votes\" on the final output, with their votes weighted by connection strengths learned from training data.\n",
    "\n",
    "5. EXAMPLE - Climate Change:\n",
    "Start with the greenhouse effect basics: certain gases (CO2, methane) trap heat like glass in a greenhouse. Explain how human activities increase these gases' concentration, enhancing the natural greenhouse effect. Use the analogy of adding blankets to a bed - each additional blanket (greenhouse gas) makes it harder for heat to escape. Connect this to observable changes: rising temperatures, sea levels, and extreme weather events.\n",
    "\n",
    "6. EXAMPLE - DNA Replication:\n",
    "Compare DNA replication to unzipping a zipper and creating two new zippers, each with one old side and one new side. Detail how helicase \"unzips\" the double helix, then polymerase adds complementary nucleotides to each strand. Emphasize the importance of base pairing (A-T, C-G) and how this ensures accurate copying. Use the analogy of a document being photocopied, where each half serves as a template for rebuilding the whole.\n",
    "\n",
    "7. EXAMPLE - Black Holes:\n",
    "Describe black holes as regions where gravity is so strong that nothing, not even light, can escape beyond the event horizon. Use the analogy of a river flowing toward a waterfall - past a certain point (event horizon), the current is too strong for anything to swim back upstream. Explain how black holes form from collapsed massive stars and how they affect space-time, like a heavy ball creating a deep depression in a stretched rubber sheet.\n",
    "\n",
    "8. EXAMPLE - Blockchain:\n",
    "Compare blockchain to a public ledger that everyone can see and verify but nobody can alter without consensus. Each block contains transactions and links to the previous block (like a chain of sealed evidence bags in a criminal case). Explain how mining works through the analogy of a complex puzzle that requires significant effort to solve but is easy to verify once solved. Emphasize how this system creates trust without requiring a central authority.\n",
    "\n",
    "9. EXAMPLE - Immune System:\n",
    "Describe the immune system as a sophisticated defense network with multiple layers: physical barriers (skin, mucus), general defenders (white blood cells), and specialized units (antibodies). Use the analogy of a castle's defense system with walls (barriers), guards (innate immunity), and specially trained soldiers (adaptive immunity). Explain how vaccines work by training these defenses to recognize specific threats before they become dangerous.\n",
    "\n",
    "10. EXAMPLE - Evolution:\n",
    "Explain natural selection through the analogy of a sieve: environmental pressures \"filter out\" less advantageous traits while allowing beneficial ones to pass through to future generations. Describe how random mutations provide the raw material for selection, like a lottery where most tickets (mutations) lose but occasional winners spread through the population. Emphasize the timescale involved and how small changes accumulate over many generations.\n",
    "\n",
    "[Examples 11-20 continue with similarly detailed explanations of: Quantum Entanglement, The Theory of Plate Tectonics, The Human Digestive System, The Big Bang Theory, Machine Learning Algorithms, Cellular Respiration, Wave-Particle Duality, The Human Nervous System, Chemical Bonding, and Nuclear Fusion...]\n",
    "\n",
    "For each explanation, remember to:\n",
    "1. Start with familiar concepts and build toward complex ones\n",
    "2. Use clear, vivid analogies that connect to everyday experience\n",
    "3. Break down complex processes into understandable steps\n",
    "4. Address common misconceptions proactively\n",
    "5. Provide real-world applications and examples\n",
    "6. Maintain scientific accuracy while ensuring accessibility\n",
    "7. Include relevant visualizations or diagrams when helpful\n",
    "8. Connect the topic to related fields and broader implications\n",
    "\n",
    "When responding to questions:\n",
    "- First assess the questioner's current understanding level\n",
    "- Choose appropriate analogies from their field of knowledge\n",
    "- Build explanations iteratively, checking for comprehension\n",
    "- Highlight key concepts and their interconnections\n",
    "- Anticipate and address likely follow-up questions\n",
    "- Provide concrete examples and practical applications\n",
    "- Maintain engagement through conversational tone\n",
    "- End with a clear summary of main points\"\"\"\n",
    "# (Add more text to make it >1024 tokens)\n",
    "\n",
    "# First call - should show no caching\n",
    "print(\"First call:\")\n",
    "response1 = check_openai_caching(long_system_prompt, \"Explain quantum computing\")\n",
    "\n",
    "time.sleep(2)  # Small delay between calls\n",
    "\n",
    "# Second call - should show caching\n",
    "print(\"\\nSecond call:\")\n",
    "response2 = check_openai_caching(long_system_prompt, \"Explain neural networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Anthropic Manual Prompt Caching\n",
    "\n",
    "With Anthropic, we can explicitly control what gets cached using the `cache_control` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anthropic_cached_message(system_content, user_message):\n",
    "    \"\"\"Create a message with cached system content in Anthropic\"\"\"\n",
    "    response = anthropic_client.beta.prompt_caching.messages.create(\n",
    "        model=\"claude-3-5-sonnet-latest\",\n",
    "        max_tokens=1024,\n",
    "        system=[\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": system_content,\n",
    "                \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "            }\n",
    "        ],\n",
    "        messages=[{\"role\": \"user\", \"content\": user_message}]\n",
    "    )\n",
    "    \n",
    "    # Print usage statistics\n",
    "    usage = response.usage\n",
    "    print(usage)\n",
    "    print(f\"Cache creation tokens: {usage.cache_creation_input_tokens}\")\n",
    "    print(f\"Cache read tokens: {usage.cache_read_input_tokens}\")\n",
    "    print(f\"Regular input tokens: {usage.input_tokens}\")\n",
    "    print(f\"Output tokens: {usage.output_tokens}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (should create cache):\n",
      "PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=1348, input_tokens=12, output_tokens=385)\n",
      "Cache creation tokens: 0\n",
      "Cache read tokens: 1348\n",
      "Regular input tokens: 12\n",
      "Output tokens: 385\n",
      "\n",
      "Second call (should use cache):\n",
      "PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=1348, input_tokens=14, output_tokens=285)\n",
      "Cache creation tokens: 0\n",
      "Cache read tokens: 1348\n",
      "Regular input tokens: 14\n",
      "Output tokens: 285\n"
     ]
    }
   ],
   "source": [
    "# Test Anthropic caching - using the same system prompt as the OpenAI example\n",
    "long_context = long_system_prompt\n",
    "\n",
    "print(\"First call (should create cache):\")\n",
    "response1 = create_anthropic_cached_message(long_context, \"Analyze this text.\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"\\nSecond call (should use cache):\")\n",
    "response2 = create_anthropic_cached_message(long_context, \"Summarize the main points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cost Analysis\n",
    "\n",
    "Let's create a function to calculate the cost savings from caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_savings(cached_tokens, model=\"claude-3-5-sonnet-latest\"):\n",
    "    \"\"\"Calculate cost savings from using cached tokens\"\"\"\n",
    "    prices = {\n",
    "        \"claude-3-5-sonnet-latest\": {\n",
    "            \"base_input\": 0.003,  # $3/MTok\n",
    "            \"cache_read\": 0.0003,  # $0.30/MTok\n",
    "        },\n",
    "        \"gpt-4o\": {\n",
    "            \"base_input\": 0.0025,  # $2.50/MTok\n",
    "            \"cache_read\": 0.00125,  # $1.25/MTok\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    model_prices = prices[model]\n",
    "    base_cost = cached_tokens * model_prices[\"base_input\"]\n",
    "    cached_cost = cached_tokens * model_prices[\"cache_read\"]\n",
    "    savings = base_cost - cached_cost\n",
    "    \n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Base cost for {cached_tokens} tokens: ${base_cost:.4f}\")\n",
    "    print(f\"Cost with caching: ${cached_cost:.4f}\")\n",
    "    print(f\"Savings: ${savings:.4f} ({(savings/base_cost)*100:.1f}%)\")\n",
    "    \n",
    "    return savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic Claude 3.5 Sonnet:\n",
      "Model: claude-3-5-sonnet-latest\n",
      "Base cost for 10000 tokens: $30.0000\n",
      "Cost with caching: $3.0000\n",
      "Savings: $27.0000 (90.0%)\n",
      "\n",
      "OpenAI GPT-4o:\n",
      "Model: gpt-4o\n",
      "Base cost for 10000 tokens: $25.0000\n",
      "Cost with caching: $12.5000\n",
      "Savings: $12.5000 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# Example cost analysis for 10,000 cached tokens\n",
    "print(\"Anthropic Claude 3.5 Sonnet:\")\n",
    "anthropic_savings = calculate_cost_savings(10000, \"claude-3-5-sonnet-latest\")\n",
    "\n",
    "print(\"\\nOpenAI GPT-4o:\")\n",
    "openai_savings = calculate_cost_savings(10000, \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "1. **Minimum Token Requirements**:\n",
    "   - OpenAI: >1,024 tokens\n",
    "   - Anthropic: >1,024 tokens (Sonnet/Opus) or >2,048 tokens (Haiku)\n",
    "\n",
    "2. **Cache Lifetime**:\n",
    "   - OpenAI: 5-10 minutes\n",
    "   - Anthropic: 5 minutes\n",
    "\n",
    "3. **Optimization Strategies**:\n",
    "   - Put static content (instructions, context, examples) at the beginning\n",
    "   - Keep dynamic content separate from cached content\n",
    "   - Monitor cache hit rates to optimize your implementation\n",
    "\n",
    "4. **Common Use Cases**:\n",
    "   - Long system prompts with examples\n",
    "   - Document analysis with consistent context\n",
    "   - Multi-turn conversations with stable history\n",
    "   - Code analysis with repository context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
