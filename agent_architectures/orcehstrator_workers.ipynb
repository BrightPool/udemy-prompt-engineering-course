{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: **Orchestrator-workers**\n",
    "\n",
    "This workflow begins with an LLM breaking down the task into subtasks that are dynamically determined based on the input. \n",
    "\n",
    "These subtasks are then processed in parallel by multiple worker LLMs. Finally, the orchestrator LLM synthesizes the workers' outputs into the final result.\n",
    "\n",
    "\n",
    "<img src=\"../images/orchestrator-workers.webp\" alt=\"Orchestrator Workers\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **Use Cases:**\n",
    "\n",
    "- Breaking down a coding problem into subtasks, using an LLM to generate code for each subtask, and making a final LLM call to combine the results into a complete solution.\n",
    "- Searching for data across multiple sources, using an LLM to identify relevant sources, and synthesizing the findings into a cohesive answer.\n",
    "- Creating a tutorial by splitting each section into subtasks like writing an introduction, outlining steps, and generating examples. Worker LLMs handle each part, and the orchestrator combines them into a polished final document.\n",
    "- Dividing a data analysis task into subtasks like cleaning the data, identifying trends, and generating visualizations. Each step is handled by separate worker LLMs, and the orchestrator integrates their findings into a complete analytical report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pydantic nest-asyncio --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-xxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL=\"gpt-4o\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtask(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the subtask\")\n",
    "    description: str = Field(..., description=\"Description of the subtask\")\n",
    "\n",
    "class OrchestratorOutput(BaseModel):\n",
    "    objective: str = Field(..., description=\"Summary of the coding task\")\n",
    "    subtasks: List[Subtask] = Field(..., description=\"List of subtasks needed to solve the coding task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define three main prompts:\n",
    " 1. **`ORCHESTRATOR_PROMPT`**: Tells the LLM to analyse the coding problem and break it down into subtasks in JSON format.\n",
    " 2. **`WORKER_PROMPT`**: Instructs the LLM worker to produce code for a given subtask.\n",
    "3. **`AGGREGATOR_PROMPT`**: Instructs a final LLM call to merge all worker outputs into a single solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "You are a skilled software architect. \n",
    "Read the coding problem below and:\n",
    "1. Summarise the objective in your own words.\n",
    "2. Identify 2-3 essential subtasks required to solve the problem.\n",
    "3. Provide your answer in JSON format with fields:\n",
    "   - objective (string)\n",
    "   - subtasks (an array of objects, each having \"name\" and \"description\")\n",
    "\n",
    "Coding Problem:\n",
    "{problem}\n",
    "\n",
    "Return only valid JSON. Do not include any additional text.\n",
    "\"\"\"\n",
    "\n",
    "WORKER_PROMPT = \"\"\"\n",
    "You are a seasoned software engineer. \n",
    "Here is your subtask for the larger coding problem:\n",
    "Subtask Name: {name}\n",
    "Subtask Description: {description}\n",
    "\n",
    "Write the Python code that accomplishes this subtask. \n",
    "Return only your code without any Markdown formatting or additional explanation.\n",
    "\"\"\"\n",
    "\n",
    "AGGREGATOR_PROMPT = \"\"\"\n",
    "You are an experienced integrator of code. \n",
    "We have code snippets from different subtasks. \n",
    "Your job is to assemble them into a cohesive, working solution that solves the original coding problem in its entirety.\n",
    "\n",
    "Subtasks Code:\n",
    "{subtasks_code}\n",
    "\n",
    "Combine these snippets into a final, complete Python solution. \n",
    "You can reorder or modify the code slightly if needed for correct integration. \n",
    "Return only the combined code without any Markdown or extra commentary.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_orchestrator(problem: str, model: str = \"gpt-4o\") -> OrchestratorOutput:\n",
    "    \"\"\"\n",
    "    Calls the orchestrator LLM to break down the coding task into subtasks.\n",
    "    Expects a valid JSON response that matches OrchestratorOutput structure.\n",
    "    \"\"\"\n",
    "    prompt = ORCHESTRATOR_PROMPT.format(problem=problem)\n",
    "    \n",
    "    response = await client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=500,\n",
    "        response_format=OrchestratorOutput\n",
    "    )\n",
    "    if not response.choices[0].message.content or not response.choices[0].message.parsed:\n",
    "        raise ValueError(\"Failed to parse orchestrator JSON.\")\n",
    "    else:\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "async def call_worker(name: str, description: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"\n",
    "    Calls a worker LLM to produce code for a given subtask.\n",
    "    Returns only the code as a string.\n",
    "    \"\"\"\n",
    "    prompt = WORKER_PROMPT.format(name=name, description=description)\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    if not response.choices[0].message.content:\n",
    "        raise ValueError(\"Failed to parse worker code.\")\n",
    "    else:\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "async def call_aggregator(subtasks_code: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"\n",
    "    Calls the aggregator LLM to merge multiple code snippets into a final solution.\n",
    "    \"\"\"\n",
    "    prompt = AGGREGATOR_PROMPT.format(subtasks_code=subtasks_code)\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "    \n",
    "    if not response.choices[0].message.content:\n",
    "        raise ValueError(\"Failed to parse aggregator code.\")\n",
    "    else:\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Orchestrator-Workers Flow\n",
    " \n",
    "We use the above helper functions in a coordinated workflow:\n",
    "1. **Orchestrator**: Breaks the main problem into subtasks.\n",
    "2. **Parallel Workers**: Each subtask is handled by a separate LLM call in parallel.\n",
    "3. **Aggregator**: Combines the results into the final code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def orchestrator_workers_flow(problem: str) -> str:\n",
    "    \"\"\"\n",
    "    Full orchestrator-workers flow:\n",
    "    1. Call the orchestrator to extract subtasks.\n",
    "    2. For each subtask, call a worker LLM in parallel to generate code.\n",
    "    3. Aggregate all resulting code snippets into a final solution.\n",
    "    4. Return that final integrated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Orchestrator analyses the problem and returns subtasks\n",
    "    orchestrator_result = await call_orchestrator(problem)\n",
    "    \n",
    "    # Print for demonstration\n",
    "    print(\"=== ORCHESTRATOR RESPONSE ===\")\n",
    "    print(\"Objective:\", orchestrator_result.objective)\n",
    "    for i, sub in enumerate(orchestrator_result.subtasks, start=1):\n",
    "        print(f\"Subtask {i} Name:\", sub.name)\n",
    "        print(f\"Subtask {i} Description:\", sub.description)\n",
    "    \n",
    "    # Step 2: Call workers in parallel\n",
    "    tasks = []\n",
    "    for subtask in orchestrator_result.subtasks:\n",
    "        tasks.append(call_worker(subtask.name, subtask.description))\n",
    "    \n",
    "    worker_results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Print intermediate worker code results\n",
    "    print(\"\\n=== WORKER CODE SNIPPETS ===\")\n",
    "    for i, code_snippet in enumerate(worker_results, start=1):\n",
    "        print(f\"--- Subtask {i} Code ---\\n{code_snippet}\\n\")\n",
    "    \n",
    "    # Prepare code snippets for aggregation\n",
    "    all_code = \"\\n\".join(worker_results)\n",
    "    \n",
    "    # Step 3: Aggregate into a final solution\n",
    "    final_solution = await call_aggregator(all_code)\n",
    "    \n",
    "    # Return the integrated code\n",
    "    return final_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ORCHESTRATOR RESPONSE ===\n",
      "Objective: Develop a Python script to read, process, and summarize data from a CSV file into JSON format, while managing missing values and identifying anomalies.\n",
      "Subtask 1 Name: Read CSV File\n",
      "Subtask 1 Description: Implement functionality to read data from a CSV file using Python libraries like pandas or csv.\n",
      "Subtask 2 Name: Handle Missing Values\n",
      "Subtask 2 Description: Incorporate methods to detect and manage missing values in the dataset, ensuring the script can process incomplete data without errors.\n",
      "Subtask 3 Name: Identify and Highlight Anomalies\n",
      "Subtask 3 Description: Develop logic to detect data anomalies, such as outliers or inconsistencies, and ensure these are highlighted in the JSON output.\n",
      "\n",
      "=== WORKER CODE SNIPPETS ===\n",
      "--- Subtask 1 Code ---\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def read_csv_file(file_path):\n",
      "    data = pd.read_csv(file_path)\n",
      "    return data\n",
      "```\n",
      "\n",
      "--- Subtask 2 Code ---\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def handle_missing_values(df):\n",
      "    # Detect missing values\n",
      "    print(\"Missing values before handling: \")\n",
      "    print(df.isnull().sum())\n",
      "\n",
      "    # Handle missing values\n",
      "    # For simplicity, we will fill missing values with mean for numerical columns\n",
      "    # and mode for categorical columns. More sophisticated methods may be used depending on the dataset.\n",
      "    for column in df.columns:\n",
      "        if df[column].dtype == np.number:\n",
      "            df[column].fillna(df[column].mean(), inplace=True)\n",
      "        else:\n",
      "            df[column].fillna(df[column].mode()[0], inplace=True)\n",
      "\n",
      "    # Check if there are still missing values\n",
      "    print(\"Missing values after handling: \")\n",
      "    print(df.isnull().sum())\n",
      "\n",
      "    return df\n",
      "```\n",
      "\n",
      "--- Subtask 3 Code ---\n",
      "```python\n",
      "import json\n",
      "import numpy as np\n",
      "\n",
      "def detect_anomalies(data):\n",
      "    anomalies = []\n",
      "    data_points = data['data_points']\n",
      "    values = [point['value'] for point in data_points]\n",
      "    mean = np.mean(values)\n",
      "    std_dev = np.std(values)\n",
      "\n",
      "    for point in data_points:\n",
      "        if abs(point['value'] - mean) > 2 * std_dev:\n",
      "            point['is_anomaly'] = True\n",
      "            anomalies.append(point)\n",
      "        else:\n",
      "            point['is_anomaly'] = False\n",
      "\n",
      "    data['anomalies'] = anomalies\n",
      "    return data\n",
      "\n",
      "# Load JSON data\n",
      "with open('data.json') as f:\n",
      "    data = json.load(f)\n",
      "\n",
      "# Detect anomalies\n",
      "data = detect_anomalies(data)\n",
      "\n",
      "# Save JSON data with anomalies highlighted\n",
      "with open('data_with_anomalies.json', 'w') as f:\n",
      "    json.dump(data, f)\n",
      "```\n",
      "\n",
      "=== FINAL AGGREGATED SOLUTION ===\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import json\n",
      "\n",
      "def read_csv_file(file_path):\n",
      "    data = pd.read_csv(file_path)\n",
      "    return data\n",
      "\n",
      "def handle_missing_values(df):\n",
      "    # Detect missing values\n",
      "    print(\"Missing values before handling: \")\n",
      "    print(df.isnull().sum())\n",
      "\n",
      "    # Handle missing values\n",
      "    # For simplicity, we will fill missing values with mean for numerical columns\n",
      "    # and mode for categorical columns. More sophisticated methods may be used depending on the dataset.\n",
      "    for column in df.columns:\n",
      "        if df[column].dtype == np.number:\n",
      "            df[column].fillna(df[column].mean(), inplace=True)\n",
      "        else:\n",
      "            df[column].fillna(df[column].mode()[0], inplace=True)\n",
      "\n",
      "    # Check if there are still missing values\n",
      "    print(\"Missing values after handling: \")\n",
      "    print(df.isnull().sum())\n",
      "\n",
      "    return df\n",
      "\n",
      "def detect_anomalies(data):\n",
      "    anomalies = []\n",
      "    data_points = data['data_points']\n",
      "    values = [point['value'] for point in data_points]\n",
      "    mean = np.mean(values)\n",
      "    std_dev = np.std(values)\n",
      "\n",
      "    for point in data_points:\n",
      "        if abs(point['value'] - mean) > 2 * std_dev:\n",
      "            point['is_anomaly'] = True\n",
      "            anomalies.append(point)\n",
      "        else:\n",
      "            point['is_anomaly'] = False\n",
      "\n",
      "    data['anomalies'] = anomalies\n",
      "    return data\n",
      "\n",
      "# Load CSV data\n",
      "data = read_csv_file('data.csv')\n",
      "\n",
      "# Handle missing values\n",
      "data = handle_missing_values(data)\n",
      "\n",
      "# Convert DataFrame to dictionary to match the format expected by detect_anomalies\n",
      "data_dict = data.to_dict('records')\n",
      "\n",
      "# Detect anomalies\n",
      "data_with_anomalies = detect_anomalies({'data_points': data_dict})\n",
      "\n",
      "# Save JSON data with anomalies highlighted\n",
      "with open('data_with_anomalies.json', 'w') as f:\n",
      "    json.dump(data_with_anomalies, f)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    # Define a sample coding problem\n",
    "    coding_problem = \"\"\"\n",
    "    Create a Python script that reads a CSV file, processes the data, \n",
    "    and outputs a summary in JSON format. \n",
    "    The solution should handle missing values gracefully and highlight data anomalies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run the orchestrator-workers flow\n",
    "    final_code = await orchestrator_workers_flow(coding_problem)\n",
    "    \n",
    "    print(\"=== FINAL AGGREGATED SOLUTION ===\")\n",
    "    print(final_code)\n",
    "\n",
    "# Execute the main function\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
