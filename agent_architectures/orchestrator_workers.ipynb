{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: **Orchestrator-workers**\n",
    "\n",
    "This workflow begins with an LLM breaking down the task into subtasks that are dynamically determined based on the input. \n",
    "\n",
    "These subtasks are then processed in parallel by multiple worker LLMs. Finally, the orchestrator LLM synthesizes the workers' outputs into the final result.\n",
    "\n",
    "\n",
    "<img src=\"../images/orchestrator-workers.webp\" alt=\"Orchestrator Workers\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **Use Cases:**\n",
    "\n",
    "- Breaking down a coding problem into subtasks, using an LLM to generate code for each subtask, and making a final LLM call to combine the results into a complete solution.\n",
    "- Searching for data across multiple sources, using an LLM to identify relevant sources, and synthesizing the findings into a cohesive answer.\n",
    "- Creating a tutorial by splitting each section into subtasks like writing an introduction, outlining steps, and generating examples. Worker LLMs handle each part, and the orchestrator combines them into a polished final document.\n",
    "- Dividing a data analysis task into subtasks like cleaning the data, identifying trends, and generating visualizations. Each step is handled by separate worker LLMs, and the orchestrator integrates their findings into a complete analytical report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pydantic nest-asyncio --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-xxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL=\"gpt-4o\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtask(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the subtask\")\n",
    "    description: str = Field(..., description=\"Description of the subtask\")\n",
    "\n",
    "class OrchestratorOutput(BaseModel):\n",
    "    objective: str = Field(..., description=\"Summary of the coding task\")\n",
    "    subtasks: List[Subtask] = Field(..., description=\"List of subtasks needed to solve the coding task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define three main prompts:\n",
    " 1. **`ORCHESTRATOR_PROMPT`**: Tells the LLM to analyse the coding problem and break it down into subtasks in JSON format.\n",
    " 2. **`WORKER_PROMPT`**: Instructs the LLM worker to produce code for a given subtask.\n",
    "3. **`AGGREGATOR_PROMPT`**: Instructs a final LLM call to merge all worker outputs into a single solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "You are a skilled software architect. \n",
    "Read the coding problem below and:\n",
    "1. Summarise the objective in your own words.\n",
    "2. Identify 2-3 essential subtasks required to solve the problem.\n",
    "3. Provide your answer in JSON format with fields:\n",
    "   - objective (string)\n",
    "   - subtasks (an array of objects, each having \"name\" and \"description\")\n",
    "\n",
    "Coding Problem:\n",
    "{problem}\n",
    "\n",
    "Return only valid JSON. Do not include any additional text.\n",
    "\"\"\"\n",
    "\n",
    "WORKER_PROMPT = \"\"\"\n",
    "You are a seasoned software engineer. \n",
    "Here is your subtask for the larger coding problem:\n",
    "Subtask Name: {name}\n",
    "Subtask Description: {description}\n",
    "\n",
    "Write the Python code that accomplishes this subtask. \n",
    "Return only your code without any Markdown formatting or additional explanation.\n",
    "\"\"\"\n",
    "\n",
    "AGGREGATOR_PROMPT = \"\"\"\n",
    "You are an experienced integrator of code. \n",
    "We have code snippets from different subtasks. \n",
    "Your job is to assemble them into a cohesive, working solution that solves the original coding problem in its entirety.\n",
    "\n",
    "Subtasks Code:\n",
    "{subtasks_code}\n",
    "\n",
    "Combine these snippets into a final, complete Python solution. \n",
    "You can reorder or modify the code slightly if needed for correct integration. \n",
    "Return only the combined code without any Markdown or extra commentary.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_orchestrator(problem: str, model: str = \"gpt-4o\") -> OrchestratorOutput:\n",
    "    \"\"\"\n",
    "    Calls the orchestrator LLM to break down the coding task into subtasks.\n",
    "    Expects a valid JSON response that matches OrchestratorOutput structure.\n",
    "    \"\"\"\n",
    "    prompt = ORCHESTRATOR_PROMPT.format(problem=problem)\n",
    "    \n",
    "    response = await client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=500,\n",
    "        response_format=OrchestratorOutput\n",
    "    )\n",
    "    if not response.choices[0].message.content or not response.choices[0].message.parsed:\n",
    "        raise ValueError(\"Failed to parse orchestrator JSON.\")\n",
    "    else:\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "async def call_worker(name: str, description: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"\n",
    "    Calls a worker LLM to produce code for a given subtask.\n",
    "    Returns only the code as a string.\n",
    "    \"\"\"\n",
    "    prompt = WORKER_PROMPT.format(name=name, description=description)\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    if not response.choices[0].message.content:\n",
    "        raise ValueError(\"Failed to parse worker code.\")\n",
    "    else:\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "async def call_aggregator(subtasks_code: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"\n",
    "    Calls the aggregator LLM to merge multiple code snippets into a final solution.\n",
    "    \"\"\"\n",
    "    prompt = AGGREGATOR_PROMPT.format(subtasks_code=subtasks_code)\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "    \n",
    "    if not response.choices[0].message.content:\n",
    "        raise ValueError(\"Failed to parse aggregator code.\")\n",
    "    else:\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Orchestrator-Workers Flow\n",
    " \n",
    "We use the above helper functions in a coordinated workflow:\n",
    "1. **Orchestrator**: Breaks the main problem into subtasks.\n",
    "2. **Parallel Workers**: Each subtask is handled by a separate LLM call in parallel.\n",
    "3. **Aggregator**: Combines the results into the final code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def orchestrator_workers_flow(problem: str) -> str:\n",
    "    \"\"\"\n",
    "    Full orchestrator-workers flow:\n",
    "    1. Call the orchestrator to extract subtasks.\n",
    "    2. For each subtask, call a worker LLM in parallel to generate code.\n",
    "    3. Aggregate all resulting code snippets into a final solution.\n",
    "    4. Return that final integrated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Orchestrator analyses the problem and returns subtasks\n",
    "    orchestrator_result = await call_orchestrator(problem)\n",
    "    \n",
    "    # Print for demonstration\n",
    "    print(\"=== ORCHESTRATOR RESPONSE ===\")\n",
    "    print(\"Objective:\", orchestrator_result.objective)\n",
    "    for i, sub in enumerate(orchestrator_result.subtasks, start=1):\n",
    "        print(f\"Subtask {i} Name:\", sub.name)\n",
    "        print(f\"Subtask {i} Description:\", sub.description)\n",
    "    \n",
    "    # Step 2: Call workers in parallel\n",
    "    tasks = []\n",
    "    for subtask in orchestrator_result.subtasks:\n",
    "        tasks.append(call_worker(subtask.name, subtask.description))\n",
    "    \n",
    "    worker_results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Print intermediate worker code results\n",
    "    print(\"\\n=== WORKER CODE SNIPPETS ===\")\n",
    "    for i, code_snippet in enumerate(worker_results, start=1):\n",
    "        print(f\"--- Subtask {i} Code ---\\n{code_snippet}\\n\")\n",
    "    \n",
    "    # Prepare code snippets for aggregation\n",
    "    all_code = \"\\n\".join(worker_results)\n",
    "    \n",
    "    # Step 3: Aggregate into a final solution\n",
    "    final_solution = await call_aggregator(all_code)\n",
    "    \n",
    "    # Return the integrated code\n",
    "    return final_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Define a sample coding problem\n",
    "    coding_problem = \"\"\"\n",
    "    Create a Python script that reads a CSV file, processes the data, \n",
    "    and outputs a summary in JSON format. \n",
    "    The solution should handle missing values gracefully and highlight data anomalies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run the orchestrator-workers flow\n",
    "    final_code = await orchestrator_workers_flow(coding_problem)\n",
    "    \n",
    "    print(\"=== FINAL AGGREGATED SOLUTION ===\")\n",
    "    print(final_code)\n",
    "\n",
    "# Execute the main function\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
