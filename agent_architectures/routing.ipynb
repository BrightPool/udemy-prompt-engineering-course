{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: **Routing**\n",
    "\n",
    "\n",
    "A workflow where user input is classified and directs to a specific task (this can be a specific LLM or a function).\n",
    "\n",
    "This allows you to optimize for many inputs in isolation.\n",
    "\n",
    "<img src=\"../images/routing.webp\" alt=\"Prompt Chaining\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **Use Cases:**\n",
    "\n",
    "- Routing easy/common questions to smaller models like Llama 3.1 8B and hard/unusual questions to more capable models like Deepseek v3 and Llama 3.3 70B to optimize cost and speed.\n",
    "- Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.\n",
    "- Different LLMs or model configurations excel at different tasks (e.g., writing summaries vs. generating code). Using a router, you can automatically detect the user's intent and send the input to the best-fit model.\n",
    "- Evaluating whether a request meets certain guidelines or triggers specific filters (e.g., checking if content is disallowed). Based on the classification, forward it to the appropriate next LLM call or step.\n",
    "- If one model's output doesn't meet a certain confidence threshold or fails for some reason, route automatically to a fallback model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pydantic --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-xxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_as_spam(user_query: str) -> bool:\n",
    "    spam_prompt = f'''You are a spam classifier that takes in a user query and returns True if the user query is spam and False otherwise.\n",
    "    The user query is: {user_query}'''\n",
    "    spam_response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": spam_prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=1000,\n",
    "        response_format=bool\n",
    "    )\n",
    "    if spam_response.choices[0].message.content is None:\n",
    "        raise ValueError(\"Spam response is None\")\n",
    "    return spam_response.choices[0].message.content\n",
    "\n",
    "def generate_summary(user_query: str) -> str:\n",
    "    summary_prompt = f'''You are a summary generator that takes in a user query and returns a summary of the user query.\n",
    "    The user query is: {user_query}'''\n",
    "    summary_response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    if summary_response.choices[0].message.content is None:\n",
    "        raise ValueError(\"Summary response is None\")\n",
    "    return summary_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(BaseModel):\n",
    "    destination: Literal[\"classify_as_spam\", \"generate_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(user_query: str) -> Router:\n",
    "    router_prompt = f'''You are a router that takes in a user query and returns the destination of the user query.\n",
    "    The destinations are \"divide\" and \"multiply\".\n",
    "    The user query is: {user_query}'''\n",
    "    \n",
    "    router_response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": router_prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=1000,\n",
    "        response_format=Router\n",
    "    )\n",
    "\n",
    "    if router_response.choices[0].message.content is None or router_response.choices[0].message.parsed.destination is None:\n",
    "        raise ValueError(\"Router response is None\")\n",
    "    \n",
    "    return router_response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A data engineer at this role level is responsible for implementing data flows between operational, analytics, and BI systems, documenting source-to-target mappings, re-engineering manual data flows for scalability, supporting data streaming systems, writing optimized ETL scripts, developing reusable business intelligence reports, and building accessible data for analysis.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    user_query = '''I want to generate a summary of this: \n",
    "    A data engineer delivers the designs set by more senior members of the data engineering community.\n",
    "\n",
    "    At this role level, you will:\n",
    "    implement data flows to connect operational systems, data for analytics and business intelligence (BI) systems\n",
    "    document source-to-target mappings\n",
    "    re-engineer manual data flows to enable scaling and repeatable use\n",
    "    support the build of data streaming systems\n",
    "    write ETL (extract, transform, load) scripts and code to ensure the ETL process performs optimally\n",
    "    develop business intelligence reports that can be reused\n",
    "    build accessible data for analysis\n",
    "    '''\n",
    "\n",
    "    router_choice = router(user_query)\n",
    "    if router_choice.destination == \"classify_as_spam\":\n",
    "        print(classify_as_spam(user_query))\n",
    "    elif router_choice.destination == \"generate_summary\":\n",
    "        print(generate_summary(user_query))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_summary\n"
     ]
    }
   ],
   "source": [
    "print(router_choice.destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
