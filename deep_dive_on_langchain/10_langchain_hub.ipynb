{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6lrCR47CJYW"
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain_openai faiss-cpu langchainhub --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jz0D8aoCJYX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JokeEbdQCJYY"
      },
      "outputs": [],
      "source": [
        "from langchain import hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAzz5QyxCJYY"
      },
      "outputs": [],
      "source": [
        "prompt = hub.pull(\"homanp/question-answer-pair\")\n",
        "prompt_two = hub.pull(\"gitmaxd/synthetic-training-data\")\n",
        "prompt_three = hub.pull(\"rlm/text-to-sql\")\n",
        "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr0mItosCJYY",
        "outputId": "c938156f-0a5c-49f8-db90-8c0528d2351d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'data_format', 'number_of_pairs'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'data_format', 'number_of_pairs'], template='You are an AI assistant tasked with generating question and answer pairs for the given context using the given format. Only answer in the format with no other text. You should create the following number of question/answer pairs: {number_of_pairs}. Return the question/answer pairs as a Python List. Each dict in the list should have the full context provided, a relevant question to the context and an answer to the question.\\n\\nFormat:\\n{data_format}\\n\\nContext:\\n{context}\\n'))])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR1uHGmQCJYZ",
        "outputId": "1ab188b0-e15e-43f8-cd9b-b4766b23b5cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['EXAMPLE', 'NUMBER', 'PERSPECTIVE', 'SEED_CONTENT'], template='Utilize Natural Language Processing techniques and Generative AI to create new Question/Answer pair textual training data for OpenAI LLMs by drawing inspiration from the given seed content: {SEED_CONTENT} \\n\\nHere are the steps to follow:\\n\\n1. Examine the provided seed content to identify significant and important topics, entities, relationships, and themes. You should use each important topic, entity, relationship, and theme you recognize. You can employ methods such as named entity recognition, content summarization, keyword/keyphrase extraction, and semantic analysis to comprehend the content deeply.\\n\\n2. Based on the analysis conducted in the first step, employ a generative language model to generate fresh, new synthetic text samples. These samples should cover the same topic, entities, relationships, and themes present in the seed data. Aim to generate {NUMBER} high-quality variations that accurately explore different Question and Answer possibilities within the data space.\\n\\n3. Ensure that the generated synthetic samples exhibit language diversity. Vary elements like wording, sentence structure, tone, and complexity while retaining the core concepts. The objective is to produce diverse, representative data rather than repetitive instances.\\n\\n4. Format and deliver the generated synthetic samples in a structured Pandas Dataframe suitable for training and machine learning purposes.\\n\\n5. The desired output length is roughly equivalent to the length of the seed content.\\n\\nCreate these generated synthetic samples as if you are writing from the {PERSPECTIVE} perspective.\\n\\nOnly output the resulting dataframe in the format of this example:  {EXAMPLE}\\n\\nDo not include any commentary or extraneous casualties.')"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_two"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hDoETPFCJYZ"
      },
      "source": [
        "## RAG Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSVTZ5KNCJYZ",
        "outputId": "46a424fd-87d6-46b1-d0bb-b84dbaea862b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCy2QK21CJYZ",
        "outputId": "d42a0a9e-93fb-4882-9cd1-e95a960ef5e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
          ]
        }
      ],
      "source": [
        "print(rag_prompt.messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9IoIaZbCJYa",
        "outputId": "7343e5ee-65b1-416b-a64e-48ee3068e204"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The approaches to task decomposition include using LLM with simple prompting, task-specific instructions, and human inputs.'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load docs\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "# Store splits\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# RAG prompt\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "question = \"What are the approaches to Task Decomposition?\"\n",
        "result = qa_chain.invoke({\"query\": question})\n",
        "result[\"result\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}