{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Expression Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langchain langchain_openai langchain-community --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnablePassthrough,\n",
        "    RunnableParallel,\n",
        "    RunnableLambda,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Accessing Previous Values using RunnablePassThrough\n",
        "\n",
        "A runnable to passthrough inputs unchanged or with additional keys.\n",
        "\n",
        "This runnable behaves almost like the identity function, except that it can be configured to add additional keys to the output, if the input is a dict.\n",
        "\n",
        "The examples below demonstrate this runnable works using a few simple chains. The chains rely on simple lambdas to make the examples easy to execute and experiment with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runnable = RunnableParallel(\n",
        "    origin=RunnablePassthrough(),\n",
        "    modified=lambda x: x+1\n",
        ")\n",
        "\n",
        "print(runnable.invoke(1)) # {'origin': 1, 'modified': 2}\n",
        "\n",
        "\n",
        "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
        "    return prompt + \" world\"\n",
        "\n",
        "chain = RunnableLambda(fake_llm) | {\n",
        "    'original': RunnablePassthrough(), # Original LLM output\n",
        "    'parsed': lambda text: text[::-1] # Parsing logic\n",
        "}\n",
        "\n",
        "chain.invoke('hello') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Prompt + Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat = ChatOpenAI()\n",
        "prompt = ChatPromptTemplate.from_template('Tell me a joke about {topic}')\n",
        "\n",
        "chain = prompt | chat\n",
        "print(chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"first\", chain.first)\n",
        "print(\"last\", chain.last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stream:\n",
        "print('\\n\\nStream:\\n')\n",
        "for s in chain.stream({\"topic\": \"bears\"}):\n",
        "    print(s.content, end=\"\", flush=True)\n",
        "\n",
        "# Invoke:\n",
        "print('\\n\\nInvoke:\\n')\n",
        "print(chain.invoke({\"topic\": \"bears\"}).content)\n",
        "\n",
        "# Batch:\n",
        "print('\\n\\nBatch:\\n')\n",
        "print(chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"bears\"}, {\"topic\": \"bears\"}]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Retrieval Augmented Generation (RAG) in LCEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langchain openai faiss-cpu tiktoken --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.vectorstores.faiss import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_texts(\n",
        "    [\"James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData\", \"James has an age of 31 years old.\"], embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# It's the same as this, but the tuple allows for line breaks:\n",
        "# {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain.invoke(\"What company does James phoenix work at?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain.invoke(\"What is James Phoenix's age?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Understanding How `itemgetter` Works with Piping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = {\n",
        "    \"data\": ['This is a test', 'Another entry...']\n",
        "}\n",
        "\n",
        "print(itemgetter(test))\n",
        "print(itemgetter('data')(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How does it work within the context of LCEL?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template('''What is the profession of James Phoenix? His profession is {profession}.''')\n",
        "\n",
        "first_chain = RunnableParallel(\n",
        "    name=lambda x: \"James Phoenix\",\n",
        "    age=lambda x: 31\n",
        ")\n",
        "\n",
        "second_chain = {\n",
        "    # itemgetter is used to get the value from the dictionary from the previous step: (note this is only the previous step, not the whole chain)\n",
        "    'name': itemgetter('name'),\n",
        "    'age': itemgetter('age'),\n",
        "    # You can not use string values, either use itemgetter or a lambda, or RunnablePassthrough\n",
        "    'profession': lambda x: \"Data Engineer\"\n",
        "}\n",
        "\n",
        "chain = first_chain | second_chain |  prompt |  ChatOpenAI() | StrOutputParser()\n",
        "chain.invoke({})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
