{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLMs (large language models)\n",
    "\n",
    "There are several methodologies for evaluating the results of LLMs. Each with their own set of trade offs and advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain_openai numpy pandas openai ipywidgets tqdm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Programmatic Rule-based Evaluation**:\n",
    "    - **Description**: This method involves setting specific, objective rules for evaluating Large Language Model (LLM) outputs. For instance, if the task is to write a blog post, a straightforward metric could be setting a minimum word count of 300 words.\n",
    "    - **Advantages**: Quick feedback loop and cost-effective, as it does not require extensive resources.\n",
    "    - **Disadvantages**: Developing metrics can be challenging for complex tasks, especially those with nuanced or subjective criteria.\n",
    "    - **When to Use**: Ideal for tasks with easily definable rules that significantly improve LLM accuracy.\n",
    "\n",
    "2. **LLM-Based Evaluation**:\n",
    "    - **Description**: Uses another LLM to assess the quality of a previous LLM's output, particularly for subjective attributes like helpfulness or readability. This approach leverages the LLM as a classifier to determine the quality of results.\n",
    "    - **Advantages**: Effective for subjective or nuanced criteria, eliminating the need for manual metric creation.\n",
    "    - **Disadvantages**: Increased evaluation time and cost due to the use of additional LLMs.\n",
    "    - **When to Use**: Suitable when rule-based methods fall short, especially for nuanced or context-rich tasks. Also it's useful for bootstrapping data for fine-tuning, as demonstrated when labeling training data with GPT-4 before comparing it with outputs from a model like Mistral.\n",
    "\n",
    "3. **Human-Based Evaluation**:\n",
    "    - **Description**: Relies on human judgment to assess LLM outputs, either through simple approval/disapproval ratings or by choosing the best result from multiple options.\n",
    "    - **Advantages**: Offers the highest accuracy due to human intelligence, reducing the likelihood of false positives and negatives (excluding human error).\n",
    "    - **Disadvantages**: Prone to human error, and is the most expensive and time-consuming method due to the need for substantial human input.\n",
    "    - **When to Use**: Best for high-stakes tasks requiring utmost accuracy. Using a data labeling service (e.g., on AWS, Azure, or GCP) can reduce time but at a higher cost.\n",
    "\n",
    "These methods allow you to tailor your evaluation strategy to your specific needs, ensuring that your LLM produces results that are both accurate and relevant to your audience. By understanding the strengths and limitations of each approach, you can choose the most effective method for your particular project, making you the master of your LLM's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Enhancing Evaluation Strategies with Combined Techniques\n",
    "\n",
    "Combining different evaluation methods can optimize your results. Start with an LLM-based evaluation, such as using GPT-4 for initial labeling. This approach rapidly generates a baseline of labels and helps in comparing various models or prompts against a set standard.\n",
    "\n",
    "Next, introduce human oversight. Have experts review the labels created by GPT-4, focusing on identifying and correcting any inaccuracies, such as false positives or negatives. This step ensures a higher level of precision in your evaluation process.\n",
    "\n",
    "## Minimizing Human Error for Superior Accuracy\n",
    "\n",
    "To achieve even greater accuracy and minimize human error, consider a multi-evaluator approach. Instead of relying on a single individual's judgment for each label, involve three reviewers and use the majority decision (at least 2 out of 3 agreeing) as the final label. This method significantly reduces the likelihood of errors.\n",
    "\n",
    "Moreover, refining the guidelines for evaluators can make a substantial difference. Clear, well-defined criteria help reviewers make more confident and accurate decisions, especially in ambiguous cases. Enhancing the clarity and distinction between different classes or outcomes in your guidelines can lead to a noticeable decrease in false positives and negatives, elevating the overall quality of your evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections below, you'll find all three of these techniques shown using mocked data and the simplest implementation to help build your intuition with evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Programmatic Rule-based Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_results = [\n",
    "    \"Coding is fun.\",\n",
    "    \"I am learning about coding and I wanted to know how to write a for loop, so I decided to learn JavaScript\",\n",
    "    \"Using strong types in code is the way forward #dev\",\n",
    "    \"learning about data engineering is fun and I had a great time at the conference! #devOps\",\n",
    "    \"Have you ever considered that learning DevOps might be useful for your career? See this thread #DevOps #Software\",\n",
    "    \"I love making software #dev\",\n",
    "    \"I love my dog. #dogs\",\n",
    "    \"Cats are awesome. #cats\",\n",
    "    \"It's always good to be with your family and friends.\"\n",
    "    \"Software is awesome #software\"\n",
    "    \"Data science is really fun and I can't wait to share what I've learned with you at DataScienceCon! #data\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social media post text length:\n",
    "social_post_df = pd.DataFrame(\n",
    "    {\"generated_social_media_post\": llm_results}  # Adding a toy dataset for LLM results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make 2x rule based evaluation metrics for your social media posts for X. You'll define that good social media posts must contain:\n",
    "- At least 1 hashtag.\n",
    "- Have be greater or equal to 30 characters, but not more than 200 characters (demonstrating a sufficient length).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_has_hashtag(text: str) -> bool:\n",
    "    if \"#\" in text:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def eval_length_of_social_post(text: str) -> bool:\n",
    "    if len(text) >= 30 and len(text) <= 150:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "# This is used for one-hot encoding the boolean results into an integer \n",
    "# so that it can be counted when calculating accuracy.\n",
    "def convert_boolean_to_one_hot_encoding(bool_result: bool) -> int:\n",
    "    if bool_result: return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to evaluate the dataset and find out the accuracy rate for each eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all of your evaluation functions here:\n",
    "eval_functions = {\n",
    "    \"eval_has_hashtag\": eval_has_hashtag,\n",
    "    \"eval_length_of_social_post\": eval_length_of_social_post,\n",
    "}\n",
    "\n",
    "# For each row, loop through:\n",
    "for index, row in social_post_df.iterrows():\n",
    "    # For each eval, run the eval and store the output in a new column for that row:\n",
    "    for key, value in eval_functions.items():\n",
    "        # Find the right eval function:\n",
    "        eval_function_to_call = eval_functions[key]\n",
    "\n",
    "        # Call the eval function and save to a column in the original df:\n",
    "        eval_result = eval_function_to_call(row[\"generated_social_media_post\"])\n",
    "\n",
    "        # Save to column at index position,\n",
    "        # this also converts the boolean to 1 for success and 0 for failure on the evaluation:\n",
    "        social_post_df.loc[index, key] = convert_boolean_to_one_hot_encoding(\n",
    "            eval_result\n",
    "        )\n",
    "\n",
    "social_post_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the accuracy for each evaluation column within the `pandas` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_columns = [col for col in social_post_df.columns if \"eval\" in col] \n",
    "print(eval_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_accuracy_results = {}\n",
    "\n",
    "for col in eval_columns:\n",
    "    # Get that eval column:\n",
    "    single_eval_results = social_post_df[col]\n",
    "\n",
    "    # Compare to the length and calculate the accuracy:\n",
    "    single_eval_accuracy = single_eval_results.sum() / len(single_eval_results)\n",
    "\n",
    "    # Save to the eval_accuracy_results dictionary:\n",
    "    eval_accuracy_results[col] = single_eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_accuracy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Blending Multiple Evaluation Metrics into a Single Score\n",
    "\n",
    "If you want to optimize across multiple metrics, you can:\n",
    "- Calculate the mean across multiple evaluation metrics to get a balanced result.\n",
    "- Weight multiple evaluation metrics by a certain percentage, depending upon what metrics are more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at calculating the mean of the two previous evals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_evals_accuracy = np.mean(list(eval_accuracy_results.values()))\n",
    "print(mean_evals_accuracy) # 61% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate weighting multiple evaluation metrics by different percentages based on their importance, let's assume we have two evaluation metrics: `eval_has_hashtag` and `eval_length_of_social_post`. We'll assign a weight of 70% to `eval_has_hashtag` (as it might be more critical for social media engagement) and 30% to `eval_length_of_social_post`.\n",
    "\n",
    "Here's a simple example illustrating this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for each evaluation metric\n",
    "weights = {\n",
    "    \"eval_has_hashtag\": 0.7,  # 70% weight\n",
    "    \"eval_length_of_social_post\": 0.3,  # 30% weight\n",
    "}\n",
    "\n",
    "# Initialize a variable to store the weighted average\n",
    "weighted_avg_accuracy = 0\n",
    "\n",
    "# Calculate the weighted average accuracy\n",
    "for eval_metric, accuracy in eval_accuracy_results.items():\n",
    "    weighted_avg_accuracy += accuracy * weights[eval_metric]\n",
    "\n",
    "print(f\"The weighted average accuracy: {weighted_avg_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the final `weighted_avg_accuracy` represents the overall accuracy of your LLM results, taking into account the relative importance of each evaluation metric. This method provides a more nuanced view of performance, especially when some aspects of the evaluation are prioritized over others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LLM Based Evaluation\n",
    "\n",
    "Let's assume you wanted to figure out if some of the social media posts generated were on the topic of coding/software. Using a known list of keywords would suffice, however it would likely not contain all of the known coding keywords and wouldn't generalise for different languages.\n",
    "\n",
    "Instead let's create a GPT-4 classifer that's responsible for identifying whether the _topic is related to coding_. \n",
    "\n",
    "First, you'll add all of the true labels to the `pandas` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the true labels to the dataframe (based on whether the topic is coding or not):\n",
    "true_labels = [1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
    "social_post_df[\"is_coding_true_labels\"] = true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 1. Define the model:\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
    ")\n",
    "\n",
    "# 2. Define the prompt:\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Act as a content editor, you are responsible for classifying\n",
    "            social media posts using the following format instructions.\n",
    "            Format Instructions: {format_instructions}\n",
    "              \"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"\"\"Social media post text: {generated_social_media_post}\n",
    "            The topic to be classified against is: {topic}\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Define the pydantic model:\n",
    "class SocialMediaPostClassifier(BaseModel):\n",
    "    is_topic: int = Field(\n",
    "        default=0,\n",
    "        description=\"\"\"This field is a classification result for \n",
    "                           whether a result is identified against a known topic. 1 for yes and 0 for no.\"\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "# 4. Define the output parser:\n",
    "output_parser = PydanticOutputParser(pydantic_object=SocialMediaPostClassifier)\n",
    "\n",
    "# 5. Create an LCEL chain:\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 6. Invoke the chain for the whole dataset:\n",
    "results = []\n",
    "TOPIC = \"coding, software or data science\"\n",
    "\n",
    "# For each row, loop through:\n",
    "for index, row in social_post_df.iterrows():\n",
    "    result = chain.invoke(\n",
    "        {\n",
    "            \"generated_social_media_post\": row[\"generated_social_media_post\"],\n",
    "            \"format_instructions\": output_parser.get_format_instructions(),\n",
    "            \"topic\": TOPIC\n",
    "        }\n",
    "    )\n",
    "    # Extract the is_topic property from the Pydantic model:\n",
    "    results.append(result.is_topic)\n",
    "\n",
    "# Save the results to a new column:\n",
    "social_post_df['eval_is_coding_topic'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's calculate the accuracy of the LLM generated evaluation against the known true labels from the `is_coding_true_labels` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each row and calculate the accuracy against the true labels:\n",
    "true_labels = social_post_df[\"is_coding_true_labels\"]\n",
    "eval_results = social_post_df[\"eval_is_coding_topic\"]\n",
    "\n",
    "# Loop through and check for equality in terms of 0 or 1 for each row, when it is equal, add 1 to the count:\n",
    "count = 0\n",
    "for index, row in social_post_df.iterrows():\n",
    "    if row[\"is_coding_true_labels\"] == row[\"eval_is_coding_topic\"]:\n",
    "        count += 1\n",
    "\n",
    "single_eval_accuracy = count / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GPT-4 achieved {round(single_eval_accuracy, 2) * 100} % accuracy against the true labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate Ground Truth Data with GPT-4 API\n",
    "\n",
    "Creating the known labels or ground truth data can be time consuming and expensive. You can use GPT-4 to _generate the ground truth data_ for you. This is useful for training your own models, and for evaluating the performance of other models. Then you can use these evals to test whether the open source or smaller / faster / cheaper models are performing as well as the larger / slower / more expensive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transactions dataset and only look at 20 transactions:\n",
    "df = pd.read_csv(\"transaction_data_with_expanded_descriptions.csv\")[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through the dataset using GPT-4 to correctly classify the transactions\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal, Union\n",
    "\n",
    "# 1. Define the model:\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"You are are an expert at analyzing bank transactions, \n",
    "you will be categorising a single transaction.\n",
    "Format Instructions:{format_instructions}\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Transaction Text:{transaction}\"\"\"\n",
    "\n",
    "# 2. Define the prompt:\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            user_prompt,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. Define the pydantic model:\n",
    "class EnrichedTransactionInformation(BaseModel):\n",
    "    transaction_type: Union[\n",
    "        Literal[\"Purchase\", \"Withdrawal\", \"Deposit\", \"Bill Payment\", \"Refund\"], None\n",
    "    ]\n",
    "    transaction_category: Union[\n",
    "        Literal[\"Food\", \"Entertainment\", \"Transport\", \"Utilities\", \"Rent\", \"Other\"],\n",
    "        None,\n",
    "    ]\n",
    "\n",
    "\n",
    "# 4. Define the output parser:\n",
    "output_parser = PydanticOutputParser(pydantic_object=EnrichedTransactionInformation)\n",
    "\n",
    "# 5. Create an LCEL chain:\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 6. Invoke the chain for the whole dataset:\n",
    "results = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    transaction = row[\"Transaction Description\"]\n",
    "    result = chain.invoke(\n",
    "        {\n",
    "            \"transaction\": transaction,\n",
    "            \"format_instructions\": output_parser.get_format_instructions(),\n",
    "        }\n",
    "    )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Add the results to the dataframe, as columns transaction type and transaction category\n",
    "transaction_types = []\n",
    "transaction_categories = []\n",
    "\n",
    "for result in results:\n",
    "    transaction_types.append(result.transaction_type)\n",
    "    transaction_categories.append(result.transaction_category)\n",
    "\n",
    "df[\"transaction_type\"] = transaction_types\n",
    "df[\"transaction_category\"] = transaction_categories\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the Accuracy of GPT-3.5 Turbo API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the model:\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
    ")\n",
    "\n",
    "# 2. Invoke the chain:\n",
    "chain = prompt | model | output_parser\n",
    "results = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    transaction = row[\"Transaction Description\"]\n",
    "    try:\n",
    "        result = chain.invoke(\n",
    "            {\n",
    "                \"transaction\": transaction,\n",
    "                \"format_instructions\": output_parser.get_format_instructions(),\n",
    "            }\n",
    "        )\n",
    "        results.append(result)\n",
    "    except:\n",
    "        # If there is an error, return None for both transaction type and transaction category\n",
    "        result = \"\"\"{\"transaction_type\": null, \"transaction_category\": null}\"\"\"\n",
    "        results.append(output_parser.parse(result))\n",
    "\n",
    "# 3. Add the results to the dataframe, as columns transaction type and transaction category\n",
    "transaction_types = []\n",
    "transaction_categories = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(i, result)\n",
    "\n",
    "    transaction_types.append(result.transaction_type)\n",
    "    transaction_categories.append(result.transaction_category)\n",
    "\n",
    "df[\"gpt-3.5_transaction_type\"] = transaction_types\n",
    "df[\"gpt-3.5_transaction_category\"] = transaction_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After classifying the transactions with GPT-3.5-Turbo, you can evaluate the accuracy against the previously generated ground truth from GPT-4 by looping through the dataframe and comparing each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the dataframe and compare the transaction type and transaction category columns:\n",
    "transaction_type_counter = 0\n",
    "transaction_category_counter = 0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row[\"gpt-3.5_transaction_type\"] == row[\"transaction_type\"]:\n",
    "        transaction_type_counter += 1\n",
    "    if row[\"gpt-3.5_transaction_category\"] == row[\"transaction_category\"]:\n",
    "        transaction_category_counter += 1\n",
    "\n",
    "print(\"GPT-3.5-Turbo Transaction Type Accuracy: \", transaction_type_counter / len(df))\n",
    "print(\n",
    "    \"GPT-3.5-Turbo Transaction Category Accuracy: \",\n",
    "    transaction_category_counter / len(df),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "## Human Based Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human-based evaluation involves direct input from individuals to assess the effectiveness of LLM outputs. This can be done through a simple approval system, where a human evaluator gives a thumbs up for satisfactory results and a thumbs down for unsatisfactory ones. Alternatively, evaluators can choose the most suitable result from a set of multiple LLM-generated options.\n",
    "\n",
    "To illustrate this, consider an example where images are generated based on a bank customer's personal interests. In a Jupyter Notebook environment, these images can be displayed alongside an interactive thumbs up/thumbs down system. Evaluators can then easily rate each image according to its relevance and appeal in relation to the customer's profile. This method provides a straightforward and intuitive way to gauge the LLM's performance in creating personalized content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "client = OpenAI()\n",
    "standard_chat_model = ChatOpenAI()\n",
    "\n",
    "image_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Act as a graphic prompt designer for a bank. You are responsible for creating a visual prompt that will be used in a marketing email to a customer for a new mortgage product.\n",
    "            The prompt will be transformed into an image using Dalle and then sent to the customer.\n",
    "\n",
    "            The bank is now offering a new 4% mortgage deal, your task is to create a visual prompt guide that uses the customer's interests to\n",
    "            personalize the mortgage offer that we're sending to them.\n",
    "\n",
    "            You must follow the following principles:\n",
    "            - Write a short concise, visual prompt that is easy to understand.\n",
    "            - Make sure that the visual prompt is personalized to the customer's profile and their interests.\n",
    "            - Avoid including any sensitive information in the prompt.\n",
    "            - Make sure that the prompt is not offensive or discriminatory in any way.\n",
    "            - Ensure that the prompt doesn't contain any text in terms of what is written on the image. This is important because the text will be added later. Focus on the type of house and the surrounding area.\n",
    "            - Use the customers location to determine the type of house and the surrounding area.\n",
    "            - Use customer interests to populate the house and the surrounding area (only if the customer has relevant interests, i.e. add in mountains if the customer is interested in hiking and so on for other hobbies).\n",
    "\n",
    "            You have access to the following context:\n",
    "            Customer Information: {customer_information}\n",
    "            ------------------\n",
    "            Customer Profile: {customer_profile}\n",
    "            ------------------\n",
    "            Response: \n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def generate_dalle_image(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a dalle image from a prompt and returns the path to the image.\n",
    "    \"\"\"\n",
    "    import base64\n",
    "    import os\n",
    "    import uuid\n",
    "\n",
    "    # if there's no /out folder, create it\n",
    "    if not os.path.exists(\"./out\"):\n",
    "        os.makedirs(\"./out\")\n",
    "\n",
    "    # generate the image\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        response_format=\"b64_json\",\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    image_data = response.data[0].b64_json\n",
    "\n",
    "    # download the image\n",
    "    image_path = f\"./out/{uuid.uuid4()}.png\"\n",
    "    with open(image_path, \"wb\") as f:\n",
    "        f.write(base64.b64decode(image_data))\n",
    "\n",
    "    return image_path\n",
    "\n",
    "\n",
    "image_chain = (\n",
    "    {\n",
    "        \"customer_information\": RunnablePassthrough(),\n",
    "        \"customer_profile\": RunnablePassthrough(),\n",
    "    }\n",
    "    | image_prompt\n",
    "    | standard_chat_model\n",
    "    | StrOutputParser()\n",
    "    | generate_dalle_image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_profiles = [\n",
    "    {\n",
    "        \"profile\": \"Outdoor Enthusiast\",\n",
    "        \"information\": \"Loves hiking, owns two dogs, enjoys gardening, resides in a rural area near mountains.\",\n",
    "    },\n",
    "    {\n",
    "        \"profile\": \"Urban Professional\",\n",
    "        \"information\": \"Works in finance, enjoys city life, likes modern architecture, lives in a downtown area.\",\n",
    "    },\n",
    "    {\n",
    "        \"profile\": \"Beach Lover\",\n",
    "        \"information\": \"Enjoys surfing, beach volleyball, tropical climate, lives near the coast.\",\n",
    "    },\n",
    "    {\n",
    "        \"profile\": \"Art and Culture Aficionado\",\n",
    "        \"information\": \"Enjoys visiting art galleries, attending theater shows, lives in a culturally rich urban area.\",\n",
    "    },\n",
    "    {\n",
    "        \"profile\": \"Family-Oriented\",\n",
    "        \"information\": \"Has three children, enjoys family activities, lives in a suburban area with good schools.\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for customer_id, profile in enumerate(good_profiles, start=1):\n",
    "    image_path = image_chain.invoke(\n",
    "        {\n",
    "            \"customer_information\": profile[\"information\"],\n",
    "            \"customer_profile\": profile[\"profile\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Collect all of the data within a dataframes:\n",
    "    results.append(\n",
    "        {\n",
    "            \"customer_id\": customer_id,\n",
    "            \"customer_information\": profile[\"information\"],\n",
    "            \"customer_profile\": profile[\"profile\"],\n",
    "            \"image_path\": image_path,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Convert the results to a DataFrame:\n",
    "image_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "## The Interactive Approval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import os\n",
    "\n",
    "response_index = 0\n",
    "image_df[\"approved\"] = pd.Series(dtype=\"str\")  # Adding a new column to store feedback\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    global response_index\n",
    "    user_feedback = 1 if b.description == \"\\U0001F44D\" else 0\n",
    "    image_df.at[response_index, \"approved\"] = user_feedback\n",
    "\n",
    "    response_index += 1\n",
    "    if response_index < len(image_df):\n",
    "        update_response()\n",
    "    else:\n",
    "        if not os.path.exists(\"data\"):\n",
    "            os.makedirs(\"data\")\n",
    "        image_df.to_csv(\"data/checked_customer_marketing_images.csv\", index=False)\n",
    "        approved_df = image_df[image_df[\"approved\"] == 1]\n",
    "        print(f\"Number of approved images: {len(approved_df)}\")\n",
    "\n",
    "\n",
    "def update_response():\n",
    "    if response_index >= len(image_df):\n",
    "        new_response = \"<p>No more responses</p>\"\n",
    "    else:\n",
    "        next_image = image_df.iloc[response_index]\n",
    "        image_html = (\n",
    "            f'<img src=\"{next_image[\"image_path\"]}\" style=\"max-width: 400px;\"/>'\n",
    "            if next_image[\"image_path\"]\n",
    "            else \"No Image\"\n",
    "        )\n",
    "        new_response = f\"{image_html}<p></p>\"\n",
    "    response.value = new_response\n",
    "    count_label.value = f\"Response: {response_index + 1} / {len(image_df)}\"\n",
    "\n",
    "\n",
    "response = widgets.HTML()\n",
    "count_label = widgets.Label()\n",
    "\n",
    "update_response()\n",
    "\n",
    "thumbs_up_button = widgets.Button(description=\"\\U0001F44D\")\n",
    "thumbs_up_button.on_click(on_button_clicked)\n",
    "\n",
    "thumbs_down_button = widgets.Button(description=\"\\U0001F44E\")\n",
    "thumbs_down_button.on_click(on_button_clicked)\n",
    "\n",
    "button_box = widgets.HBox([thumbs_down_button, thumbs_up_button])\n",
    "\n",
    "display(response, button_box, count_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Here's a breakdown of the key components of this code:\n",
    "\n",
    "1. **Initialization and DataFrame Update**:\n",
    "   - The `response_index` variable is initialized to track the current image being evaluated.\n",
    "   - A new column, `approved`, is added to the `image_df` DataFrame to store the evaluation feedback for each image.\n",
    "\n",
    "2. **Button Click Event Handling**:\n",
    "   - The `on_button_clicked` function updates the `approved` status of the current image based on the button clicked (thumbs up or thumbs down).\n",
    "   - It increments the `response_index` to move to the next image and calls `update_response` to refresh the displayed image.\n",
    "\n",
    "3. **Response Update and Display**:\n",
    "   - The `update_response` function dynamically updates the content displayed to the evaluator. It shows the current image (if available) and updates the progress label (`count_label`).\n",
    "   - If all images have been evaluated, it displays a message indicating no more responses are left.\n",
    "\n",
    "4. **Data Saving and Summary**:\n",
    "   - Once all images have been evaluated, the system saves the feedback to a CSV file. It also calculates and displays the number of images approved.\n",
    "\n",
    "5. **Interactive Widgets Setup**:\n",
    "   - Two buttons (`thumbs_up_button` and `thumbs_down_button`) are created and linked to the `on_button_clicked` event handler.\n",
    "   - These buttons allow the evaluator to approve or disapprove each image.\n",
    "   - The `response` and `count_label` widgets are used to display the current image and the evaluation progress, respectively.\n",
    "\n",
    "6. **Displaying the Interactive Elements**:\n",
    "   - Finally, the interactive elements (image, buttons, and labels) are displayed in a horizontal box layout using `display(response, button_box, count_label)`.\n",
    "\n",
    "This interactive approval system is an efficient tool for quickly gathering human feedback on a set of images, making it a valuable component for human-based evaluations in LLM projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Enhancing Evaluation Scores for LLMs\n",
    "\n",
    "To elevate the performance of your Large Language Models (LLMs) and boost evaluation scores, consider implementing these strategies:\n",
    "\n",
    "1. **Optimized Prompts**: Employ prompt engineering techniques to refine your prompts. Thoughtfully crafted prompts can significantly improve the LLM's output quality and, in turn, its evaluation scores.\n",
    "\n",
    "2. **Model Selection**: Experiment with various models. More sophisticated models often yield better results, but don't overlook the potential of smaller models, especially in the context of prompt refinement.\n",
    "\n",
    "3. **Enhanced Retrieval Techniques**: Inject relevant context into your prompts dynamically to enhance accuracy. Improve retrieval by expanding the range of sourced documents, experimenting with different similarity metrics, and employing hybrid search strategies that combine various methods for more comprehensive results.\n",
    "\n",
    "4. **Task Decomposition with Multiple LLM Chains**: Break down complex tasks into simpler components and address each with a dedicated LLM chain. This specialized approach allows for more targeted prompt optimization and can lead to superior outcomes.\n",
    "\n",
    "5. **Fine-Tuning with Targeted Data**: If adjustments to prompts and retrieval processes fall short, consider fine-tuning your LLM with a curated, labeled dataset. This dataset can be generated through GPT-4 or human labeling, depending on your specific requirements and resources.\n",
    "\n",
    "By embracing these strategies, you can fine-tune your LLM's performance to meet and exceed evaluation benchmarks. Remember, the key to success lies in a balanced combination of technology and technique, ensuring your LLM not only meets but surpasses expectations.\n",
    "\n",
    "---\n",
    "\n",
    "In conclusion, enhancing your LLM's evaluation scores is a dynamic process that combines the art of prompt engineering with the science of model optimization. By continuously refining your approach and staying adaptable to new techniques and technologies, you can ensure your LLM delivers top-tier performance, making it a valuable asset in your toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
