{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899bfc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken openai --quiet --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a50057-c0b9-4c50-a2d6-52bea8d3613b",
   "metadata": {},
   "source": [
    "# Using Responses API with Manual Token Management\n",
    "\n",
    "This notebook demonstrates how to use the OpenAI Responses API with `store=False` (i.e. without persistent state storage) while manually managing and counting tokens using the [tiktoken](https://github.com/openai/tiktoken) package.\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "1. Sending a conversation turn using the Responses API with `store=False` and chaining subsequent messages using `previous_response_id`.\n",
    "2. Counting tokens in a list of messages using a custom function `num_tokens_from_messages`.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e934d-4ace-4bf7-8a53-421636a6d757",
   "metadata": {},
   "source": [
    "## 1. Using the Responses API with `store=False`\n",
    "\n",
    "In the following example, we use the Responses API to send a conversation turn. By setting `store=False`, the conversation is not stored automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0bd83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c87add",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4.1-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6358101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI client instance\n",
    "client = OpenAI(\n",
    "    # Replace with your actual API key or use: api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=\"YOUR_API_KEY_HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0f557-591b-45b9-887d-90ea2d4317d1",
   "metadata": {},
   "source": [
    "## Managing Tokens Manually using tiktoken and a Custom Function\n",
    "\n",
    "The following function counts the number of tokens used by a list of messages. It selects an appropriate encoding based on the provided model name and counts tokens in each message field. This is particularly useful if you need to monitor token usage to stay within your model's context window or track cost.\n",
    "\n",
    "Below is the custom function `num_tokens_from_messages` followed by an example of how to call it on a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe7ab8e1-febb-4ed7-9e96-d6ddcb9f4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "    \n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0125\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        \"gpt-4o-mini-2024-07-18\",\n",
    "        \"gpt-4o-2024-08-06\"\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0125\")\n",
    "    elif \"gpt-4o-mini\" in model:\n",
    "        print(\"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\")\n",
    "    elif \"gpt-4o\" in model:\n",
    "        print(\"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4o-2024-08-06\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"num_tokens_from_messages() is not implemented for model {model}.\")\n",
    "    \n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939c396",
   "metadata": {},
   "source": [
    "## Example of Manual Token Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ab16118",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_headings = [\n",
    "    \"I. Introduction A. Definition of the 2008 Financial Crisis B. Overview of the Causes and Effects of the Crisis C. Importance of Understanding the Crisis\",\n",
    "    \"II. Historical Background A. Brief History of the US Financial System B. The Creation of the Housing Bubble C. The Growth of the Subprime Mortgage Market\",\n",
    "    \"III. Key Players in the Crisis A. Government Entities B. Financial Institutions C. Homeowners and Borrowers\",\n",
    "    \"IV. Causes of the Crisis A. The Housing Bubble and Subprime Mortgages B. The Role of Investment Banks and Rating Agencies C. The Failure of Regulatory Agencies D. Deregulation of the Financial Industry\",\n",
    "    \"V. The Domino Effect A. The Spread of the Crisis to the Global Financial System B. The Impact on the Real Economy C. The Economic Recession\",\n",
    "    \"VI. Government Responses A. The Troubled Asset Relief Program (TARP) B. The American Recovery and Reinvestment Act C. The Dodd-Frank Wall Street Reform and Consumer Protection Act\",\n",
    "    \"VII. Effects on Financial Institutions A. Bank Failures and Bailouts B. Stock Market Decline C. Credit Freeze\",\n",
    "    \"VIII. Effects on Homeowners and Borrowers A. Foreclosures and Bankruptcies B. The Loss of Home Equity C. The Impact on Credit Scores\",\n",
    "    \"IX. Effects on the Global Economy A. The Global Financial Crisis B. The Impact on Developing Countries C. The Role of International Organizations\",\n",
    "    \"X. Criticisms and Controversies A. Bailouts for Financial Institutions B. Government Intervention in the Economy C. The Role of Wall Street in the Crisis\",\n",
    "    \"XI. Lessons Learned A. The Need for Stronger Regulation B. The Importance of Transparency C. The Need for Better Risk Management\",\n",
    "    \"XII. Reforms and Changes A. The Dodd-Frank Wall Street Reform and Consumer Protection Act B. Changes in Regulatory Agencies C. Changes in the Financial Industry\",\n",
    "    \"XIII. Current Economic Situation A. Recovery from the Crisis B. Impact on the Job Market C. The Future of the US Economy\",\n",
    "    \"XIV. Comparison to Previous Financial Crises A. The Great Depression B. The Savings and Loan Crisis C. The Dot-Com Bubble\",\n",
    "    \"XV. Economic and Social Impacts A. The Widening Wealth Gap B. The Rise of Populist Movements C. The Long-Term Effects on the Economy\",\n",
    "    \"XVI. The Role of Technology A. The Use of Technology in the Financial Industry B. The Impact of Technology on the Crisis C. The Future of the Financial Industry\",\n",
    "    \"XVII. Conclusion A. Recap of the Causes and Effects of the Crisis B. The Importance of Learning from the Crisis C. Final Thoughts\",\n",
    "    \"XVIII. References A. List of Sources B. Additional Reading C. Further Research\",\n",
    "    \"XIX. Glossary A. Key Terms B. Definitions\",\n",
    "    \"XX. Appendix A. Timeline of the Crisis B. Financial Statements of Key Players C. Statistical Data on the Crisis\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa579a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant for a financial news website. You are writing a series of articles about the 2008 financial crisis. You have been given a list of headings for each article. You need to write a short paragraph for each heading. You can use the headings as a starting point for your writing.\\n\\n\"\n",
    "system_prompt += \"All of the subheadings:\\n\"\n",
    "\n",
    "# Set up the messages list:\n",
    "messages = []\n",
    "\n",
    "# Add all of the subheadings to the system prompt to give the model context:\n",
    "for heading in article_headings:\n",
    "    system_prompt += f\"{heading}\\n\"\n",
    "\n",
    "# Append the first developer/system message to the messages list:\n",
    "messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "# This will ensure that if the token count goes over the limit, the last message will be removed, \n",
    "# to ensure that the token count is reduced as the chat history grows:\n",
    "MAX_TOKEN_SIZE = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "227cf69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current message count 3\n",
      "Current token count 848\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 5\n",
      "Current token count 1023\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 7\n",
      "Current token count 1200\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 9\n",
      "Current token count 1398\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 11\n",
      "Current token count 1574\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 13\n",
      "Current token count 1785\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 15\n",
      "Current token count 1971\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 17\n",
      "Current token count 2157\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 17\n",
      "Current token count 2175\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 17\n",
      "Current token count 2216\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 17\n",
      "Current token count 2221\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 17\n",
      "Current token count 2255\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 16\n",
      "Current token count 2227\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 16\n",
      "Current token count 2273\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 15\n",
      "Current token count 2133\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 15\n",
      "Current token count 2150\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 15\n",
      "Current token count 2128\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 15\n",
      "Current token count 2127\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 15\n",
      "Current token count 2066\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Current message count 16\n",
      "Current token count 2216\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "Removed a message to reduce token count!\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n"
     ]
    }
   ],
   "source": [
    "# Loop over all of the headings and generate a chunk for each one\n",
    "for heading in article_headings:\n",
    "    \n",
    "    # Add on a user prompt to the chat history object:\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": f\"Write a very large paragraph about {heading}. Make it very long and detailed.\"}\n",
    "    )\n",
    "\n",
    "    # Tell ChatGPT to generate a response:\n",
    "    response = client.responses.create(\n",
    "        mode=MODEL,\n",
    "        input=messages,\n",
    "        store=False\n",
    "    )\n",
    "\n",
    "    # Update the conversation history with the assistant's response\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response.output_text})\n",
    "    print(\"Current message count\", len(messages))\n",
    "    print(\"Current token count\", num_tokens_from_messages(messages))\n",
    "\n",
    "    # Whilst the Chat history object is more than 2048 tokens, remove the oldest non-system/developer message:\n",
    "    while num_tokens_from_messages(messages, model='gpt-4o-mini') > 2048:\n",
    "        \n",
    "        # Find the index of the first message that is not a system or developer message:\n",
    "        non_system_msg_index = next(\n",
    "            (i for i, msg in enumerate(messages) if msg[\"role\"] not in [\"system\", \"developer\"]), None\n",
    "        )\n",
    "\n",
    "        # If there is a non-system message, remove it:\n",
    "        if non_system_msg_index is not None:\n",
    "            messages.pop(non_system_msg_index)\n",
    "        print(\"Removed a message to reduce token count!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51efc59-6d87-426a-a9c1-e32f73dbf8ac",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We used the Responses API with `store=False` to send conversation turns and chain them manually.\n",
    "- We defined a custom function `num_tokens_from_messages` to count tokens from a list of messages with tiktoken.\n",
    "\n",
    "This approach gives you detailed control over conversation state and token management—useful for fine‑tuning agent workflows and staying within token limits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
