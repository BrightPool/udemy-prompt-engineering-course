{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf31d299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken openai --quiet --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27e0f7-dc50-4773-9f57-6c22bf53e7c3",
   "metadata": {},
   "source": [
    "# Managing Conversation State with OpenAI's API\n",
    "\n",
    "In this notebook, we'll walk through how to manually manage conversation state when interacting with the OpenAI API. We'll cover two approaches:\n",
    "\n",
    "1. **Manually managing conversation state:** Building a conversation history list and appending messages for each turn.\n",
    "2. **Chaining responses using `previous_response_id`:** This method allows the model to access prior context automatically.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b08f9-2f3c-4aa5-a6aa-56f9100dd7fa",
   "metadata": {},
   "source": [
    "## 1. Manual Conversation State Management\n",
    "\n",
    "For this first example, we'll create a simple knock-knock joke conversation by manually constructing a conversation history. Every turn in the conversation is stored in a list of messages, where each message is a dictionary containing a role (`user` or `assistant`) and the content.\n",
    "\n",
    "This way of managing state is very explicit and works well when you want fine-grained control over the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71234130",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4.1-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI client instance\n",
    "client = OpenAI(\n",
    "    # Replace with your actual API key or use: api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=\"YOUR_API_KEY_HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb82a3c-3f4d-4ea9-9ee1-df10e120eb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant response: Orange who?\n"
     ]
    }
   ],
   "source": [
    "# Construct a conversation history manually for a knock-knock joke\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"knock knock.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Orange.\"}\n",
    "]\n",
    "\n",
    "# Create a response using the conversation history\n",
    "response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=history\n",
    ")\n",
    "\n",
    "print(\"Assistant response:\", response.output_text)\n",
    "\n",
    "# (Optional) Inspect the full response object for further details\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b06a2b-640a-4a1f-8e16-219e87c81e8c",
   "metadata": {},
   "source": [
    "## 2. Updating Conversation History Across Rounds\n",
    "\n",
    "In this example, we'll start a conversation with a simple joke prompt. We'll then update the conversation history with the model's response, and ask for another joke. Updating the conversation history manually makes sure the model has full context of previous turns.\n",
    "\n",
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5421d6f-1918-4dca-9c7d-937d33142d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke: Sure! Here’s a joke for you:\n",
      "\n",
      "Why don’t skeletons fight each other?\n",
      "\n",
      "Because they don’t have the guts!\n",
      "Another joke: Of course! Here’s another one:\n",
      "\n",
      "Why was the math book sad?\n",
      "\n",
      "Because it had too many problems!\n"
     ]
    }
   ],
   "source": [
    "# Start a new conversation with an initial joke request\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"tell me a joke\"}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=history,\n",
    "    store=False  # We're not storing the conversation automatically\n",
    ")\n",
    "\n",
    "print(\"Joke:\", response.output_text)\n",
    "\n",
    "# Update the conversation history with all parts of the response\n",
    "history += [{\"role\": el.role, \"content\": el.content} for el in response.output]\n",
    "\n",
    "# Ask for another joke in the same conversation\n",
    "history.append({\"role\": \"user\", \"content\": \"tell me another\"})\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=history,\n",
    "    store=False\n",
    ")\n",
    "\n",
    "print(\"Another joke:\", second_response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cbe4c1-cf8f-4b8b-a3fb-4f4e85eec168",
   "metadata": {},
   "source": [
    "## 3. Chaining Responses with `previous_response_id`\n",
    "\n",
    "Another way to carry conversation context is to chain responses by using the `previous_response_id` parameter. With this method, you initiate a conversation and then provide additional requests that reference the previous response.\n",
    "\n",
    "Below is an example where we first ask the model for a joke and then ask it to explain why that joke is funny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4679adb-15b1-4666-b854-62520a8ca32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for a joke\n",
    "response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=\"tell me a joke\"\n",
    ")\n",
    "\n",
    "print(\"Joke:\", response.output_text)\n",
    "\n",
    "# Chain the next request using previous_response_id so the model retains context\n",
    "second_response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    previous_response_id=response.id,\n",
    "    input=[{\"role\": \"user\", \"content\": \"explain why this is funny.\"}]\n",
    ")\n",
    "\n",
    "print(\"Explanation:\", second_response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06197f15-f80c-417e-bb99-75b241ba7ffa",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "- **Managing Tokens:** Keep in mind that every input and output token (and any internal reasoning tokens) counts toward your model's context window. For long conversations, you may need to manage or truncate history.\n",
    "- **Best Practices:** When building production applications, consider automating state management so that conversations flow naturally. Using proper error handling, logging, and context management will improve your application's DX significantly.\n",
    "\n",
    "This notebook demonstrated how you can use Python to manage conversation state. Feel free to experiment with other conversation flows and adjust the approach as needed for your teaching or production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
