{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought\n",
    "\n",
    "Chain of Thought (CoT) prompting is a technique for eliciting reasoning in LLMs, by asking the model to think first before completing a task. CoT prompting has shown significant improvements in the performance of language models on tasks that require multi-step reasoning, such as math word problems, commonsense reasoning, and symbolic manipulation. By providing a step-by-step thought process, the model can better understand and solve complex problems. This technique enhances the reasoning capabilities of large language models by encouraging them to break down complex problems into intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
    "\n",
    "The CoT technique has been referenced in many AI papers since, but was first introduced by Jason Wei et al. The authors demonstrate that CoT prompting is particularly effective in few-shot learning scenarios and that its benefits scale with model size. The technique shows versatility across various tasks. Importantly, CoT prompting makes the model's reasoning process more transparent and human-like, enhancing interpretability. The process is analogous to system 2 thinking in the human brain, when a problem is solved by spending time thinking a more deliberate and structured way, as opposed to system 1 thinking which is more intuitive and instinctive (like LLMs are without CoT). The paper's findings suggest that future AI developments might focus more on prompting techniques to unlock latent abilities in existing models, rather than solely on developing larger models. While highly effective, the authors note that CoT prompting has limitations and may not be as beneficial for simpler tasks or very small models.\n",
    "\n",
    "> [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) by Wei, J., et al. (2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Prompt Result:\n",
      "A: 2\n",
      "\n",
      "Chain of Thought Prompt Result:\n",
      "Spell out RASPBERRY and count the Rs:\n",
      "\n",
      "R-A-S-P-B-E-R-R-Y\n",
      "\n",
      "There is one R at the beginning, one R in the middle, and another R towards the end. \n",
      "\n",
      "1 + 1 + 1 = 3 Rs in total.\n",
      "\n",
      "A: 3\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_completion(prompt, system):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "system = \"Solve the following problem and return the answer in the format A: <answer>\"\n",
    "# Standard prompting (one-shot)\n",
    "question = \"\"\"Q: How many Rs are there in RASPBERRY?\"\"\"\n",
    "\n",
    "standard_prompt = f\"\"\"Q: How many Es are there in ELEPHANT?\n",
    "A: 2\n",
    "Q: How many Ps are there in PINEAPPLE?\n",
    "A: 3\n",
    "Q: How many Os are there in CHOCOLATE?\n",
    "A: 2\n",
    "{question}\"\"\"\n",
    "\n",
    "print(\"Standard Prompt Result:\")\n",
    "print(get_completion(standard_prompt, system))\n",
    "\n",
    "# Chain of Thought prompting (zero-shot)\n",
    "cot_prompt = f\"\"\"Q: How many Es are there in ELEPHANT?\n",
    "Let's think step by step:\n",
    "Spell out ELEPHANT and count the Es:\n",
    "E-L-E-P-H-A-N-T\n",
    "There's one E at the beginning and one in the middle. 1 + 1 = 2 Es in total.\n",
    "A: 2\n",
    "\n",
    "Q: How many Ps are there in PINEAPPLE?\n",
    "Let's think step by step:\n",
    "Spell out PINEAPPLE and count the Ps:\n",
    "P-I-N-E-A-P-P-L-E\n",
    "There's one P at the beginning and two Ps together in the middle. 1 + 2 = 3 Ps in total.\n",
    "A: 3\n",
    "\n",
    "Q: How many Os are there in CHOCOLATE?\n",
    "Let's think step by step:\n",
    "Spell out CHOCOLATE and count the Os:\n",
    "C-H-O-C-O-L-A-T-E\n",
    "There's one O at the beginning and one in the middle. 1 + 1 = 2 Os in total.\n",
    "A: 2\n",
    "\n",
    "{question}. \n",
    "Let's think step by step:\"\"\"\n",
    "\n",
    "print(\"\\nChain of Thought Prompt Result:\")\n",
    "print(get_completion(cot_prompt, system))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the answer correct? True\n",
      "Does it provide steps? True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def evaluate_response(response, correct_answer):\n",
    "    # Check if the final answer is correct\n",
    "    final_answer = re.search(r'A:\\s*(\\d+)', response)\n",
    "\n",
    "    if final_answer:\n",
    "        answer = int(final_answer.group(1))\n",
    "        is_correct = (answer == correct_answer)\n",
    "    else:\n",
    "        is_correct = False\n",
    "    \n",
    "    # Check if steps are provided\n",
    "    lines = response.strip().split('\\n')\n",
    "    has_steps = len(lines) > 1\n",
    "    \n",
    "    return is_correct, has_steps\n",
    "\n",
    "# Test the evaluation function\n",
    "test_response = \"\"\"Q: How many Os are there in CHOCOLATE?\n",
    "A: Let's spell out CHOCOLATE and count the Os:\n",
    "C-H-O-C-O-L-A-T-E\n",
    "There's one O at the beginning and one in the middle. 1 + 1 = 2 Os in total.\n",
    "A: 2\"\"\"\n",
    "\n",
    "is_correct, has_steps = evaluate_response(test_response, 2)\n",
    "print(f\"Is the answer correct? {is_correct}\")\n",
    "print(f\"Does it provide steps? {has_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Prompting Results:\n",
      "Accuracy: 90.00%\n",
      "Average Time: 0.93 seconds\n",
      "Percentage with Steps: 0.00%\n",
      "\n",
      "Chain of Thought Prompting Results:\n",
      "Accuracy: 100.00%\n",
      "Average Time: 1.62 seconds\n",
      "Percentage with Steps: 100.00%\n"
     ]
    }
   ],
   "source": [
    "eval_set = [\n",
    "    {\n",
    "        \"Q\": \"How many As are there in HAMBURGER?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out HAMBURGER and count the As:\\nH-A-M-B-U-R-G-E-R\\nThere's only one A in the second position. So there is 1 A in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Ts are there in CATERPILLAR?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out CATERPILLAR and count the Ts:\\nC-A-T-E-R-P-I-L-L-A-R\\nThere's only one T in the third position. So there is 1 T in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Ls are there in UMBRELLA?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out UMBRELLA and count the Ls:\\nU-M-B-R-E-L-L-A\\nThere are two Ls together near the end of the word. So there are 2 Ls in total.\",\n",
    "        \"A\": 2\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Os are there in OCTOPUS?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out OCTOPUS and count the Os:\\nO-C-T-O-P-U-S\\nThere's one O at the beginning and one in the middle. 1 + 1 = 2 Os in total.\",\n",
    "        \"A\": 2\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Ns are there in SUNFLOWER?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out SUNFLOWER and count the Ns:\\nS-U-N-F-L-O-W-E-R\\nThere's only one N in the third position. So there is 1 N in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Es are there in BICYCLE?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out BICYCLE and count the Es:\\nB-I-C-Y-C-L-E\\nThere's only one E at the end of the word. So there is 1 E in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Rs are there in REFRIGERATOR?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out REFRIGERATOR and count the Rs:\\nR-E-F-R-I-G-E-R-A-T-O-R\\nThere's one R at the beginning, one in the middle, and one at the end. 1 + 1 + 1 = 3 Rs in total.\",\n",
    "        \"A\": 3\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Ss are there in PINEAPPLES?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out PINEAPPLES and count the Ss:\\nP-I-N-E-A-P-P-L-E-S\\nThere's only one S at the end of the word. So there is 1 S in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Cs are there in POPSICLE?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out POPSICLE and count the Cs:\\nP-O-P-S-I-C-L-E\\nThere's only one C in the sixth position. So there is 1 C in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many As are there in PANDA?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out PANDA and count the As:\\nP-A-N-D-A\\nThere's one A in the second position and one at the end. 1 + 1 = 2 As in total.\",\n",
    "        \"A\": 2\n",
    "    }\n",
    "]\n",
    "\n",
    "import time\n",
    "\n",
    "def run_evaluation(prompt_type):\n",
    "    correct_count = 0\n",
    "    step_count = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for example in eval_set:\n",
    "        question = example[\"Q\"]\n",
    "        correct_answer = example[\"A\"]\n",
    "\n",
    "        if prompt_type == \"standard\":\n",
    "            prompt = f\"\"\"Q: How many Es are there in ELEPHANT?\n",
    "A: 2\n",
    "Q: How many Ps are there in PINEAPPLE?\n",
    "A: 3\n",
    "Q: How many Os are there in CHOCOLATE?\n",
    "A: 2\n",
    "{question}\"\"\"\n",
    "        else:  # CoT prompt\n",
    "            prompt = f\"\"\"Q: How many Es are there in ELEPHANT?\n",
    "Let's think step by step:\n",
    "Spell out ELEPHANT and count the Es:\n",
    "E-L-E-P-H-A-N-T\n",
    "There's one E at the beginning and one in the middle. 1 + 1 = 2 Es in total.\n",
    "A: 2\n",
    "\n",
    "Q: How many Ps are there in PINEAPPLE?\n",
    "Let's think step by step:\n",
    "Spell out PINEAPPLE and count the Ps:\n",
    "P-I-N-E-A-P-P-L-E\n",
    "There's one P at the beginning and two Ps together in the middle. 1 + 2 = 3 Ps in total.\n",
    "A: 3\n",
    "\n",
    "Q: How many Os are there in CHOCOLATE?\n",
    "Let's think step by step:\n",
    "Spell out CHOCOLATE and count the Os:\n",
    "C-H-O-C-O-L-A-T-E\n",
    "There's one O at the beginning and one in the middle. 1 + 1 = 2 Os in total.\n",
    "A: 2\n",
    "\n",
    "{question}\n",
    "Let's think step by step:\"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = get_completion(prompt, system)\n",
    "        end_time = time.time()\n",
    "\n",
    "        is_correct, has_steps = evaluate_response(response, correct_answer)\n",
    "        correct_count += int(is_correct)\n",
    "        step_count += int(has_steps)\n",
    "        total_time += (end_time - start_time)\n",
    "\n",
    "    accuracy = correct_count / len(eval_set)\n",
    "    avg_time = total_time / len(eval_set)\n",
    "    step_percentage = step_count / len(eval_set) * 100\n",
    "\n",
    "    return accuracy, avg_time, step_percentage\n",
    "\n",
    "# Run evaluation for standard prompting\n",
    "standard_accuracy, standard_avg_time, standard_step_percentage = run_evaluation(\"standard\")\n",
    "\n",
    "# Run evaluation for CoT prompting\n",
    "cot_accuracy, cot_avg_time, cot_step_percentage = run_evaluation(\"cot\")\n",
    "\n",
    "# Print results\n",
    "print(\"Standard Prompting Results:\")\n",
    "print(f\"Accuracy: {standard_accuracy:.2%}\")\n",
    "print(f\"Average Time: {standard_avg_time:.2f} seconds\")\n",
    "print(f\"Percentage with Steps: {standard_step_percentage:.2f}%\")\n",
    "\n",
    "print(\"\\nChain of Thought Prompting Results:\")\n",
    "print(f\"Accuracy: {cot_accuracy:.2%}\")\n",
    "print(f\"Average Time: {cot_avg_time:.2f} seconds\")\n",
    "print(f\"Percentage with Steps: {cot_step_percentage:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
